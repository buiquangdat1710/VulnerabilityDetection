"""
- Giả sử đầu vào là "time flies like an arrow"
- Sau khi được tokenizer thì thành 
{'input_ids': tensor([[ 2051, 10029,  2066,  2019,  8612]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}
- input_ids sẽ được embedding thành [1, 5, 768], tức là 1 token sẽ được embedding thành vector có 768 chiều
----------------------------------------------------------------------------------------------------------------------------
- Kiến trúc Attention:
Đầu vào: [1,5,768]
Q, K, V được tạo ra bằng cách cho [1, 5, 768] qua tầng Linear có kích thước là (embed_dim, head_dim), ví dụ là (768, 64)
Khi đó Q,K,V = [1, 5, 64], lưu ý là theo paper gốc "Attention for all you need" thì chiều cuối cùng của V có thể khác bọn Q,K. Nhưng Q,K chiều phải giống nhau
Tính Attention = softmax(Q*K^T/ sqrt(d))*V = [1,5,64]
Đầu ra: [1,5,64]

----------------------------------------------------------------------------------------------------------------------------
- Kiến trúc của MultiHeadAttention: 
Đầu vào: [1,5,768]
Ta sẽ dùng 12 lớp Attention. Bản chất có số 64 vì 64*12 = 768. Tức ta sẽ tính bọn Attention 12 lần xong rồi concat lại theo 
chiều ngang để tạo ra [1,5,64*12] = [1,5,768]. Sau đó đưa qua tần Linear (768,768), đầu ra thành [1,5,768]
Đầu ra: [1,5,768]

----------------------------------------------------------------------------------------------------------------------------
- Kiến trúc FeedForward:
Đầu vào: [1,5,768]
Đưa đầu vào qua Linear (768, 3072) (tùy kiến trúc mà số 3072 là khác nhau)
Đưa qua RELU (paper gốc là Relu not GELU)
Đưa qua Linear (3072, 768) (tùy kiến trúc mà số 3072 là khác nhau)
Đầu ra: [1,5,768]
----------------------------------------------------------------------------------------------------------------------------
- Kiến trúc TransformerEncoderLayer (theo bài báo gốc):
Đầu vào: [1,5,768]
Đưa đầu vào qua MultiHeadAttention
Skip Connection với trước khi đưa qua MultiHeadAttention
LayerNorm
FeedForward
Skip Connection với trước khi đưa qua FeedForward
LayerNorm
Đầu ra: [1,5,768]
----------------------------------------------------------------------------------------------------------------------------
- Kiến trúc Embeddings:
Đầu vào: input_ids [1,5]
nn.Embedding có sẵn (who khow it work ? Maybe just Linear or CBOW or skipGram)
Đầu ra: [1,5,768]
----------------------------------------------------------------------------------------------------------------------------
- Kiến trúc TransformerEncoder:
Đầu vào: input_ids [1,5]
Đầu vào được đưa qua Embeddings và đưa qua 12 TransformerEncoderLayer (crazy) 
Đầu ra:  [1,5,768]
----------------------------------------------------------------------------------------------------------------------------
- Kiến trúc TransformerForSequenceClassification:
Đầu vào: input_ids [1,5]
Đầu vào được đưa qua TransformerEncoder,  qua Dropout, qua Linear (768,2)
Đầu ra:  [1,2]
"""

class AttentionHead(nn.Module):
  def __init__(self, embed_dim, head_dim):
    super().__init__()
    self.q = nn.Linear(embed_dim, head_dim)
    self.k = nn.Linear(embed_dim, head_dim)
    self.v = nn.Linear(embed_dim, head_dim)
  def forward(self, hidden_state):
    attn_outputs = scaled_dot_product_attention(self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))
    return attn_outputs


class MultiHeadAttention(nn.Module):
  def __init__(self, config):
    super().__init__()
    embed_dim = config.hidden_size
    num_head = config.num_attention_heads
    head_dim = embed_dim // num_head
    self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_head)])
    self.linear = nn.Linear(embed_dim, embed_dim)
  def forward(self,hidden_state):
    x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)
    x = self.linear(x)
    return x

class FeedForward(nn.Module):
  def __init__(self,config):
    super().__init__()
    self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size)
    self.gelu = nn.GELU()
    self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size)
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
  def  forward(self,x):
    x = self.linear1(x)
    x = self.gelu(x)
    x = self.linear2(x)
    x = self.dropout(x)
    return x


class TransformerEncoderLayer(nn.Module):
  def __init__(self, config):
    super().__init__()
    self.layernorm1 = nn.LayerNorm(config.hidden_size)
    self.multiheadattetion = MultiHeadAttention(config)
    self.layernorm2 = nn.LayerNorm(config.hidden_size)
    self.ffn = FeedForward(config)
  def forward(self, x):
    hidden_states = self.layernorm1(x)
    x = x + self.multiheadattetion(hidden_states)
    x = x + self.ffn(self.layernorm2(x))
    return x

class Embeddings(nn.Module):
  def __init__(self, config):
      super().__init__()
      self.token_embeddings = nn.Embedding(config.vocab_size,
      config.hidden_size)
      self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
      self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)
      self.dropout = nn.Dropout()
  def forward(self,input_ids):
    seq_length = input_ids.size(1)
    # print("seq_length: ", seq_length)
    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)
    # print("position_ids: ", position_ids)
    token_embeddings = self.token_embeddings(input_ids)
    # print("token_embeddings: ",token_embeddings)
    # print(token_embeddings.size())
    position_embeddings = self.position_embeddings(position_ids)
    # print("position_embeddings: ",position_embeddings)
    embeddings = token_embeddings + position_embeddings
    embeddings = self.layer_norm(embeddings)
    embeddings = self.dropout(embeddings)
    return embeddings

class TransformerEncoder(nn.Module):
  def __init__(self, config):
    super().__init__()
    self.embeddings = Embeddings(config)
    self.layers = nn.ModuleList([TransformerEncoderLayer(config)
    for _ in range(config.num_hidden_layers)])
  def forward(self, x):
    x = self.embeddings(x)
    for layer in self.layers:
      x = layer(x)
    return x

 class TransformerForSequenceClassification(nn.Module):
   def __init__(self, config):
     super().__init__()
     self.encoder = TransformerEncoder(config)
     self.dropout = nn.Dropout(config.hidden_dropout_prob)
     self.classifier = nn.Linear(config.hidden_size, config.num_labels)
   def forward(self, x):
     x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token
     x = self.dropout(x)
     x = self.classifier(x)
   return x
  
